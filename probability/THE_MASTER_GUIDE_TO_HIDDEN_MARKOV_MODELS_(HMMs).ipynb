{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# THE MASTER GUIDE TO HIDDEN MARKOV MODELS (HMMs)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 1. The Intuition (What Makes a Markov Model ‚ÄúHidden‚Äù?)\n",
        "\n",
        "A Markov Chain describes directly observable states:\n",
        "\n",
        "Sunny ‚Üí Rainy ‚Üí Sunny ‚Üí ‚Ä¶\n",
        "\n",
        "A Hidden Markov Model is different:\n",
        "\n",
        "The true states cannot be observed directly;  \n",
        "you only observe signals (emissions) produced by those states.\n",
        "\n",
        "Think of a magician behind a curtain.\n",
        "\n",
        "You cannot see the magician‚Äôs hands (the hidden states).  \n",
        "You only see the patterns of smoke, light, or sound he produces (observations).\n",
        "\n",
        "You must infer:\n",
        "\n",
        "‚Äúwhat must be happening behind the curtain‚Äù  \n",
        "based solely on the signals you observe.\n",
        "\n",
        "This is the magic of HMMs.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. The Structure of an HMM (The Ingredients of the Machine)\n",
        "\n",
        "An HMM consists of two interacting stochastic processes:\n",
        "\n",
        "---\n",
        "\n",
        "### 2.1 Hidden States (Invisible World)\n",
        "\n",
        "$$\n",
        "S = \\{ s_1, s_2, \\dots, s_N \\}\n",
        "$$\n",
        "\n",
        "These follow a Markov Chain:\n",
        "\n",
        "$$\n",
        "P(X_{t+1} = s_j \\mid X_t = s_i) = a_{ij}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 2.2 Observations (Visible World)\n",
        "\n",
        "$$\n",
        "O = \\{ o_1, o_2, \\dots, o_M \\}\n",
        "$$\n",
        "\n",
        "Each hidden state emits an observation according to:\n",
        "\n",
        "$$\n",
        "b_j(k) = P(O_t = o_k \\mid X_t = s_j)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 2.3 Initial Probabilities\n",
        "\n",
        "$$\n",
        "\\pi_i = P(X_1 = s_i)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 3. The HMM Model Components (The ‚ÄúTriple‚Äù)\n",
        "\n",
        "An HMM is fully described by:\n",
        "\n",
        "$$\n",
        "\\lambda = (A, B, \\pi)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $$A$$ = transition matrix  \n",
        "- $$B$$ = emission probabilities  \n",
        "- $$\\pi$$ = initial distribution  \n",
        "\n",
        "---\n",
        "\n",
        "## 4. The Three Fundamental Problems of HMMs\n",
        "\n",
        "Every application of HMMs revolves around solving one of these three ancient problems:\n",
        "\n",
        "---\n",
        "\n",
        "### Problem 1 ‚Äî Evaluation (‚ÄúHow likely is this sequence?‚Äù)\n",
        "\n",
        "Given model $$\\lambda$$ and observed sequence  \n",
        "$$O = (o_1, o_2, \\dots, o_T):$$\n",
        "\n",
        "$$\n",
        "P(O \\mid \\lambda)\n",
        "$$\n",
        "\n",
        "**Solution:** Forward Algorithm\n",
        "\n",
        "---\n",
        "\n",
        "### Problem 2 ‚Äî Decoding (‚ÄúWhat is the most likely hidden state path?‚Äù)\n",
        "\n",
        "Given $$O$$, find:\n",
        "\n",
        "$$\n",
        "\\arg\\max P(X \\mid O,\\lambda)\n",
        "$$\n",
        "\n",
        "**Solution:** Viterbi Algorithm\n",
        "\n",
        "---\n",
        "\n",
        "### Problem 3 ‚Äî Learning (‚ÄúHow do we adjust A, B, \\pi from data?‚Äù)\n",
        "\n",
        "Given only observations $$O$$, estimate $$\\lambda$$.\n",
        "\n",
        "**Solution:** Baum‚ÄìWelch / EM Algorithm\n",
        "\n",
        "These three problems make HMMs a complete ecosystem.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. The Equations (Elegant, Powerful, Essential)\n",
        "\n",
        "---\n",
        "\n",
        "### 5.1 Forward Algorithm (Evaluation)\n",
        "\n",
        "Define:\n",
        "\n",
        "$$\n",
        "\\alpha_t(j) = P(o_1, o_2, \\dots, o_t, X_t = s_j)\n",
        "$$\n",
        "\n",
        "Recursive formula:\n",
        "\n",
        "$$\n",
        "\\alpha_{t+1}(j) = \\left( \\sum_i \\alpha_t(i) a_{ij} \\right) b_j(O_{t+1})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 5.2 Viterbi Algorithm (Decoding)\n",
        "\n",
        "Define:\n",
        "\n",
        "$$\n",
        "\\delta_t(j) =\n",
        "\\max_{x_1,\\dots,x_{t-1}}\n",
        "P(X_t = s_j, o_1, \\dots, o_t)\n",
        "$$\n",
        "\n",
        "Recursive update:\n",
        "\n",
        "$$\n",
        "\\delta_{t+1}(j) =\n",
        "\\max_i [\\delta_t(i) a_{ij}] \\, b_j(O_{t+1})\n",
        "$$\n",
        "\n",
        "Use backpointers to recover the best path.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.3 Baum‚ÄìWelch Algorithm (Learning)\n",
        "\n",
        "Alternates between:\n",
        "\n",
        "**E-step:** Compute expected transitions and emissions using forward/backward probabilities.\n",
        "\n",
        "**M-step:** Update model:\n",
        "\n",
        "$$\n",
        "a_{ij} =\n",
        "\\frac{\\text{expected transitions from } i \\text{ to } j}\n",
        "{\\text{expected transitions out of } i}\n",
        "$$\n",
        "\n",
        "$$\n",
        "b_j(k) =\n",
        "\\frac{\\text{expected emissions of } o_k \\text{ from } s_j}\n",
        "{\\text{expected visits to } s_j}\n",
        "$$\n",
        "\n",
        "This is an expectation-maximization (EM) algorithm.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. A Simple, Creative Example of an HMM  \n",
        "### The ‚ÄúMood and Message‚Äù Problem\n",
        "\n",
        "Hidden states (we don't see these):\n",
        "\n",
        "| State | Meaning |\n",
        "|-------|---------|\n",
        "| H | Happy |\n",
        "| S | Sad |\n",
        "\n",
        "Observations (we see these):\n",
        "\n",
        "| Observation | Meaning |\n",
        "|------------|----------|\n",
        "| üòä | Smiley message |\n",
        "| üòê | Neutral message |\n",
        "| üò¢ | Sad message |\n",
        "\n",
        "---\n",
        "\n",
        "### Transition Probabilities (A)\n",
        "\n",
        "| From ‚Üí To | H | S |\n",
        "|-----------|---|---|\n",
        "| H | $$0.7$$ | $$0.3$$ |\n",
        "| S | $$0.4$$ | $$0.6$$ |\n",
        "\n",
        "This says:\n",
        "\n",
        "- Happy people tend to stay happy (70%).  \n",
        "- Sad people often stay sad (60%).\n",
        "\n",
        "---\n",
        "\n",
        "### Emission Probabilities (B)\n",
        "\n",
        "| State ‚Üí Emoji | üòä | üòê | üò¢ |\n",
        "|---------------|----|----|----|\n",
        "| H | $$0.6$$ | $$0.3$$ | $$0.1$$ |\n",
        "| S | $$0.1$$ | $$0.4$$ | $$0.5$$ |\n",
        "\n",
        "So:\n",
        "\n",
        "- Happy people mostly send üòä  \n",
        "- Sad people mostly send üò¢  \n",
        "\n",
        "---\n",
        "\n",
        "### Initial Probabilities (œÄ)\n",
        "\n",
        "$$\n",
        "\\pi = [0.5, 0.5]\n",
        "$$\n",
        "\n",
        "We assume equal chance of Happy or Sad initially.\n",
        "\n",
        "---\n",
        "\n",
        "## 6.1 Suppose You Observe This Sequence:\n",
        "\n",
        "$$\n",
        "O = [üòä, üòä, üò¢]\n",
        "$$\n",
        "\n",
        "We want to answer:\n",
        "\n",
        "- How likely is this sequence? (Forward Algorithm)  \n",
        "- What is the most likely hidden mood sequence? (Viterbi)\n",
        "\n",
        "Let us do the second ‚Äî the intuitive one.\n",
        "\n",
        "---\n",
        "\n",
        "## 6.2 Viterbi Decoding (Most Likely Mood Sequence)\n",
        "\n",
        "### Step 1: Start\n",
        "\n",
        "For observation üòä:\n",
        "\n",
        "Happy:\n",
        "\n",
        "$$\n",
        "0.5 \\times 0.6 = 0.3\n",
        "$$\n",
        "\n",
        "Sad:\n",
        "\n",
        "$$\n",
        "0.5 \\times 0.1 = 0.05\n",
        "$$\n",
        "\n",
        "Already we lean toward ‚ÄúHappy.‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Second observation = üòä\n",
        "\n",
        "From Happy ‚Üí Happy:\n",
        "\n",
        "$$\n",
        "0.3 \\times 0.7 \\times 0.6 = 0.126\n",
        "$$\n",
        "\n",
        "From Happy ‚Üí Sad:\n",
        "\n",
        "$$\n",
        "0.3 \\times 0.3 \\times 0.1 = 0.009\n",
        "$$\n",
        "\n",
        "From Sad ‚Üí Happy:\n",
        "\n",
        "$$\n",
        "0.05 \\times 0.4 \\times 0.6 = 0.012\n",
        "$$\n",
        "\n",
        "From Sad ‚Üí Sad:\n",
        "\n",
        "$$\n",
        "0.05 \\times 0.6 \\times 0.1 = 0.003\n",
        "$$\n",
        "\n",
        "The largest value is:\n",
        "\n",
        "$$\n",
        "0.126 \\Rightarrow H \\to H\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Step 3: Third observation = üò¢\n",
        "\n",
        "Happy emits üò¢ only 10%  \n",
        "Sad emits üò¢ at 50%\n",
        "\n",
        "Most likely transition:\n",
        "\n",
        "$$\n",
        "H \\to S\n",
        "$$\n",
        "\n",
        "Final decoded path:\n",
        "\n",
        "**Happy ‚Üí Happy ‚Üí Sad**\n",
        "\n",
        "This is the most probable emotional journey behind the observed messages.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. The Beautiful Summary\n",
        "\n",
        "Hidden Markov Models are systems where:\n",
        "\n",
        "- Hidden states generate visible observations  \n",
        "- The hidden states follow a Markov chain  \n",
        "- The observations follow probabilistic emission rules  \n",
        "\n",
        "HMMs answer three fundamental questions:\n",
        "\n",
        "- **Evaluation** ‚Üí How likely is this observation sequence?  \n",
        "- **Decoding** ‚Üí What hidden state sequence generated it?  \n",
        "- **Learning** ‚Üí How do we learn the model from data?  \n",
        "\n",
        "HMMs are the foundation of:\n",
        "\n",
        "- Speech recognition  \n",
        "- DNA sequencing  \n",
        "- Gesture recognition  \n",
        "- POS tagging in NLP  \n",
        "- Automatic translation (before deep learning)  \n",
        "- Finance (regime-switching models)  \n",
        "- Robotics localization  \n",
        "- Anomaly detection  \n",
        "\n",
        "For centuries, HMMs were among the most beautiful achievements of probabilistic modeling ‚Äî and they are still essential today.\n"
      ],
      "metadata": {
        "id": "K-TnO3tZu9iZ"
      }
    }
  ]
}